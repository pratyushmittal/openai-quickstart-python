{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "This is an example of applying :class:`~sklearn.decomposition.NMF` and\n",
    ":class:`~sklearn.decomposition.LatentDirichletAllocation` on a corpus\n",
    "of documents and extract additive models of the topic structure of the\n",
    "corpus.  The output is a plot of topics, each represented as bar plot\n",
    "using top few words based on weights.\n",
    "\n",
    "Non-negative Matrix Factorization is applied with two different objective\n",
    "functions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\n",
    "The latter is equivalent to Probabilistic Latent Semantic Indexing.\n",
    "\n",
    "The default parameters (n_samples / n_features / n_components) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is polynomial in NMF. In LDA, the time complexity is\n",
    "proportional to (n_samples * iterations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.817s.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data, _ = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "data_samples = data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples\n",
    "data_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding vectorization\n",
    "\n",
    "Following tutorial from https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# playing with countvector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?',\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the vectorizer has tokenized all the words (vocabullary)\n",
    "\n",
    "# this will show the actual names\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the X represents presence of each word in each document\n",
    "# We passed 4 documents, therefore X has 4 rows\n",
    "# each row contains 9 elements because there are 9 total words\n",
    "# the value 0,1,2 show the count of number of times that word is present in that document\n",
    "X.toarray()\n",
    "\n",
    "# in first document:\n",
    "# and is present 0 times, document is present 1 time, first is present 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we created a vocabulary of individual words in the above\n",
    "# it ignored relative positions of the words\n",
    "#\n",
    "# we can create a vocabulary of phrases as well by using n_grams\n",
    "# for example, an n_gram of 2 will create a dictionary of all\n",
    "# individual words as well as all 2 consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we didn't use stop words in the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding tf-idf\n",
    "\n",
    "Based on readings and examples from https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we used the above vectorisation strategy to convert a raw text\n",
    "# into a matrix of numbers\n",
    "# each row in the matrix represents the document row.\n",
    "#\n",
    "# each row has equal number of elements\n",
    "# the number of elements is the dictionary of all words\n",
    "# the number in each row represents the count of number of times the dictionary word is used in document\n",
    "#\n",
    "# now we can use this matrix of numbers (bag-of-words) to do further analysis\n",
    "#\n",
    "# tf-idf is a method to give weighted information for each term in each document\n",
    "# we use tfidf-transformer to convert the count matrix into weight matrix\n",
    "#\n",
    "# this transformation from count matrix -> to weighted matrix is done using formula:\n",
    "# term weight = term-frequency * 1 / uniqueness of term\n",
    "# i.e. terms-weight is directly proportional to its frequency\n",
    "# but inversely proportional to how frequently it is found in OTHER documents\n",
    "# this final term-weight is a value between 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(smooth_idf=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43306685, 0.56943086, 0.43306685, 0.        ,\n",
       "        0.        , 0.33631504, 0.        , 0.43306685],\n",
       "       [0.        , 0.24014568, 0.        , 0.24014568, 0.        ,\n",
       "        0.89006176, 0.18649454, 0.        , 0.24014568],\n",
       "       [0.56115953, 0.        , 0.        , 0.        , 0.56115953,\n",
       "        0.        , 0.23515939, 0.56115953, 0.        ],\n",
       "       [0.        , 0.43306685, 0.56943086, 0.43306685, 0.        ,\n",
       "        0.        , 0.33631504, 0.        , 0.43306685]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert X bag-of-words into weighted average\n",
    "tfidf = transformer.fit_transform(X)\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning to our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.173s.\n"
     ]
    }
   ],
   "source": [
    "# now data we have a sample data,\n",
    "# lets convert it to a tf-idf vector\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# exclude common English words\n",
    "# also exclude words only one document\n",
    "# or in at least 95% of the documents\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, # ignore words which are present in over 95% documents\n",
    "    min_df=2, # ignore words which are present in less than 2 documents\n",
    "    max_features=1000, # consider only the top 1000 words by frequency\n",
    "    stop_words=\"english\", # ignore common english words such as the\n",
    ")\n",
    "\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '10', '100', '11', '12', '128', '13', '130', '14',\n",
       "       '15', '16', '17', '18', '19', '1992', '1993', '20', '200', '21',\n",
       "       '22', '23', '24', '25', '250', '26', '27', '28', '29', '2nd', '30',\n",
       "       '300', '31', '32', '33', '34', '35', '36', '37', '38', '3d', '40',\n",
       "       '42', '43', '44', '45', '48', '49', '50', '500', '51', '55', '60',\n",
       "       '66', '70', '72', '75', '80', '800', '86', '90', '92', '93', '__',\n",
       "       'able', 'ac', 'accept', 'access', 'according', 'act', 'action',\n",
       "       'actually', 'add', 'added', 'addition', 'address',\n",
       "       'administration', 'advance', 'age', 'ago', 'agree', 'aids', 'air',\n",
       "       'al', 'allow', 'allowed', 'alt', 'america', 'american', 'amiga',\n",
       "       'analysis', 'anonymous', 'answer', 'answers', 'anti', 'anybody',\n",
       "       'apartment', 'appears', 'apple', 'application', 'applications',\n",
       "       'apply', 'appreciated', 'approach', 'appropriate', 'apr', 'april',\n",
       "       'archive', 'area', 'areas', 'aren', 'argument', 'armenia',\n",
       "       'armenian', 'armenians', 'army', 'article', 'ask', 'asked',\n",
       "       'asking', 'assume', 'atheism', 'attack', 'attacks', 'attempt',\n",
       "       'au', 'author', 'authority', 'available', 'average', 'away',\n",
       "       'azerbaijan', 'bad', 'based', 'basic', 'basically', 'begin',\n",
       "       'belief', 'believe', 'best', 'better', 'bible', 'big', 'bike',\n",
       "       'billion', 'bios', 'bit', 'black', 'block', 'blood', 'blue',\n",
       "       'board', 'bob', 'body', 'book', 'books', 'bought', 'box', 'brake',\n",
       "       'break', 'bring', 'btw', 'build', 'building', 'built', 'bus',\n",
       "       'business', 'buy', 'buying', 'ca', 'cable', 'called', 'calls',\n",
       "       'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars',\n",
       "       'case', 'cases', 'cause', 'cc', 'cd', 'center', 'certain',\n",
       "       'certainly', 'chance', 'change', 'changed', 'changes', 'cheap',\n",
       "       'check', 'children', 'chip', 'choice', 'christ', 'christian',\n",
       "       'christians', 'church', 'citizens', 'city', 'claim', 'clear',\n",
       "       'clearly', 'clinton', 'clipper', 'clock', 'close', 'code', 'cold',\n",
       "       'color', 'com', 'come', 'comes', 'coming', 'command', 'comments',\n",
       "       'commercial', 'common', 'communications', 'community', 'comp',\n",
       "       'company', 'complete', 'completely', 'computer', 'condition',\n",
       "       'conference', 'congress', 'connector', 'consider', 'considered',\n",
       "       'contact', 'containing', 'contains', 'continue', 'control',\n",
       "       'controller', 'copies', 'copy', 'correct', 'cost', 'costs',\n",
       "       'couldn', 'countries', 'country', 'couple', 'course', 'court',\n",
       "       'cover', 'create', 'created', 'crime', 'crowd', 'cs', 'cubs',\n",
       "       'current', 'currently', 'cut', 'dangerous', 'data', 'database',\n",
       "       'date', 'dave', 'david', 'day', 'days', 'dc', 'dead', 'deal',\n",
       "       'death', 'dec', 'decided', 'defense', 'deleted', 'department',\n",
       "       'design', 'designed', 'details', 'developed', 'development',\n",
       "       'device', 'devices', 'did', 'didn', 'die', 'died', 'difference',\n",
       "       'different', 'difficult', 'digital', 'directly', 'directory',\n",
       "       'discussion', 'disease', 'disk', 'display', 'division', 'does',\n",
       "       'doesn', 'dog', 'doing', 'dollars', 'don', 'door', 'dos', 'dot',\n",
       "       'doubt', 'dr', 'drive', 'driver', 'drivers', 'drives', 'driving',\n",
       "       'drug', 'earlier', 'early', 'earth', 'easily', 'easy', 'edu',\n",
       "       'effect', 'effective', 'effort', 'email', 'encrypted',\n",
       "       'encryption', 'end', 'energy', 'enforcement', 'engine', 'entire',\n",
       "       'equipment', 'eric', 'error', 'especially', 'events', 'evidence',\n",
       "       'exactly', 'example', 'excellent', 'exist', 'expect', 'experience',\n",
       "       'explain', 'extra', 'face', 'fact', 'fair', 'fairly', 'faith',\n",
       "       'family', 'faq', 'far', 'fast', 'faster', 'father', 'fax',\n",
       "       'feature', 'features', 'federal', 'feel', 'field', 'figure',\n",
       "       'file', 'files', 'final', 'finally', 'fine', 'firearm', 'fit',\n",
       "       'floppy', 'flyers', 'folks', 'follow', 'following', 'food',\n",
       "       'force', 'forget', 'form', 'format', 'formats', 'free', 'freedom',\n",
       "       'friend', 'ftp', 'function', 'future', 'game', 'games', 'gas',\n",
       "       'gave', 'gay', 'general', 'generally', 'gets', 'getting', 'given',\n",
       "       'gives', 'giving', 'gm', 'goal', 'god', 'goes', 'going', 'gone',\n",
       "       'good', 'got', 'gov', 'government', 'graphics', 'great', 'greek',\n",
       "       'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half',\n",
       "       'hand', 'happen', 'happened', 'happens', 'happy', 'hard',\n",
       "       'hardware', 'haven', 'having', 'head', 'heads', 'health', 'hear',\n",
       "       'heard', 'heart', 'heaven', 'held', 'hell', 'help', 'hi', 'high',\n",
       "       'higher', 'history', 'hit', 'hiv', 'hockey', 'hold', 'home',\n",
       "       'hope', 'hospital', 'hot', 'hours', 'house', 'hp', 'human', 'ibm',\n",
       "       'id', 'idea', 'ii', 'image', 'images', 'imagine', 'important',\n",
       "       'include', 'included', 'includes', 'including', 'increase',\n",
       "       'individual', 'info', 'information', 'input', 'inside', 'instead',\n",
       "       'insurance', 'interested', 'interesting', 'interface', 'internal',\n",
       "       'international', 'internet', 'involved', 'isn', 'israel',\n",
       "       'israeli', 'issue', 'james', 'jesus', 'jewish', 'jews', 'job',\n",
       "       'jobs', 'john', 'just', 'kept', 'key', 'keys', 'kill', 'killed',\n",
       "       'killing', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows',\n",
       "       'lack', 'land', 'language', 'large', 'late', 'later', 'latest',\n",
       "       'launch', 'law', 'laws', 'leafs', 'learn', 'leave', 'led', 'left',\n",
       "       'legal', 'let', 'letter', 'level', 'library', 'license', 'life',\n",
       "       'light', 'like', 'likely', 'limited', 'line', 'lines', 'list',\n",
       "       'listen', 'little', 'live', 'lives', 'living', 'll', 'local',\n",
       "       'long', 'longer', 'look', 'looking', 'looks', 'lord', 'lost',\n",
       "       'lot', 'lots', 'love', 'low', 'luck', 'lunar', 'mac', 'machine',\n",
       "       'machines', 'magi', 'mail', 'main', 'major', 'make', 'makes',\n",
       "       'making', 'mamma', 'man', 'manager', 'manual', 'mark', 'market',\n",
       "       'marriage', 'mars', 'mary', 'mass', 'master', 'math', 'matter',\n",
       "       'maybe', 'mb', 'mean', 'meaning', 'means', 'media', 'medical',\n",
       "       'member', 'members', 'memory', 'men', 'mention', 'mentioned',\n",
       "       'message', 'middle', 'mike', 'mil', 'miles', 'military', 'million',\n",
       "       'mind', 'mission', 'mit', 'mode', 'model', 'models', 'modern',\n",
       "       'money', 'monitor', 'months', 'moon', 'moral', 'mother', 'motif',\n",
       "       'mr', 'ms', 'nasa', 'national', 'nature', 'navy', 'near',\n",
       "       'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new',\n",
       "       'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal',\n",
       "       'north', 'note', 'nsa', 'number', 'numbers', 'objects', 'obvious',\n",
       "       'offer', 'office', 'oh', 'oil', 'ok', 'old', 'older', 'ones',\n",
       "       'open', 'opinion', 'opinions', 'orbit', 'order', 'org',\n",
       "       'organization', 'original', 'os', 'output', 'outside', 'package',\n",
       "       'page', 'paper', 'papers', 'parents', 'particular', 'parts',\n",
       "       'party', 'pass', 'past', 'paul', 'pay', 'pc', 'people',\n",
       "       'performance', 'period', 'person', 'personal', 'peter', 'phone',\n",
       "       'pick', 'piece', 'pin', 'pittsburgh', 'place', 'places', 'plan',\n",
       "       'play', 'played', 'player', 'players', 'playing', 'plus', 'point',\n",
       "       'points', 'police', 'policy', 'political', 'population', 'port',\n",
       "       'position', 'possible', 'post', 'posted', 'posting', 'power', 'pp',\n",
       "       'present', 'president', 'press', 'pretty', 'price', 'printer',\n",
       "       'private', 'pro', 'probably', 'probe', 'probes', 'problem',\n",
       "       'problems', 'process', 'product', 'program', 'programs', 'project',\n",
       "       'prove', 'provide', 'pub', 'public', 'purpose', 'putting',\n",
       "       'quality', 'question', 'questions', 'quite', 'radio', 'ram',\n",
       "       'range', 'rate', 'rates', 'ray', 'read', 'reading', 'real',\n",
       "       'really', 'reason', 'reasonable', 'received', 'recent', 'recently',\n",
       "       'recommend', 'record', 'red', 'reference', 'regular', 'related',\n",
       "       'release', 'religion', 'religious', 'remember', 'reply', 'report',\n",
       "       'request', 'require', 'required', 'research', 'response', 'rest',\n",
       "       'result', 'results', 'return', 'right', 'rights', 'road', 'robert',\n",
       "       'role', 'rom', 'room', 'rules', 'run', 'running', 'runs', 'safety',\n",
       "       'said', 'sale', 'san', 'save', 'saw', 'say', 'saying', 'says',\n",
       "       'school', 'sci', 'science', 'scientific', 'screen', 'scsi',\n",
       "       'search', 'season', 'second', 'section', 'secure', 'security',\n",
       "       'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial',\n",
       "       'series', 'seriously', 'server', 'service', 'set', 'sex', 'sgi',\n",
       "       'shall', 'short', 'shot', 'shots', 'shows', 'shuttle', 'signal',\n",
       "       'similar', 'simple', 'simply', 'sin', 'single', 'site',\n",
       "       'situation', 'size', 'slow', 'small', 'society', 'software',\n",
       "       'solar', 'sold', 'soldiers', 'soon', 'sorry', 'sort', 'sound',\n",
       "       'sounds', 'source', 'sources', 'soviet', 'space', 'spacecraft',\n",
       "       'special', 'specific', 'specifically', 'speed', 'st', 'standard',\n",
       "       'start', 'started', 'starting', 'state', 'statement', 'states',\n",
       "       'station', 'stay', 'stop', 'story', 'street', 'strong', 'study',\n",
       "       'stuff', 'stupid', 'subject', 'suggest', 'sun', 'support',\n",
       "       'supported', 'supports', 'suppose', 'supposed', 'sure', 'surface',\n",
       "       'switch', 'systems', 'taken', 'takes', 'taking', 'talk', 'talking',\n",
       "       'tape', 'team', 'teams', 'technical', 'technology', 'tell', 'term',\n",
       "       'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things',\n",
       "       'think', 'thinking', 'thought', 'time', 'times', 'tires', 'today',\n",
       "       'told', 'took', 'toronto', 'total', 'town', 'traffic', 'transfer',\n",
       "       'tried', 'trouble', 'true', 'trust', 'truth', 'try', 'trying',\n",
       "       'turkish', 'turn', 'turned', 'tv', 'type', 'types', 'uk',\n",
       "       'understand', 'unfortunately', 'united', 'university', 'unix',\n",
       "       'unless', 'use', 'used', 'useful', 'user', 'users', 'uses',\n",
       "       'using', 'usually', 'value', 'van', 'various', 've', 'version',\n",
       "       'vga', 'video', 'view', 'volume', 'vs', 'want', 'wanted', 'wants',\n",
       "       'war', 'washington', 'wasn', 'water', 'way', 'weapon', 'weapons',\n",
       "       'week', 'weeks', 'went', 'western', 'white', 'wife', 'willing',\n",
       "       'win', 'window', 'windows', 'wish', 'woman', 'women', 'won',\n",
       "       'wonder', 'wondering', 'word', 'words', 'work', 'worked',\n",
       "       'working', 'works', 'world', 'worse', 'worth', 'wouldn', 'write',\n",
       "       'written', 'wrong', 'xfree86', 'year', 'years', 'yes', 'young'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.08365563, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.12574832, 0.04605022,\n",
       "        0.06032677],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.187s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 3, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we might need simple (unweighted) countVector as well\n",
    "#\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\n",
    "    \"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "    \"n_samples=%d and n_features=%d...\" % (n_samples, n_features)\n",
    ")\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1, alpha=0.1, l1_ratio=0.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\n",
    ")\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
    "    \"divergence) with tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "t0 = time()\n",
    "nmf = NMF(\n",
    "    n_components=n_components,\n",
    "    random_state=1,\n",
    "    beta_loss=\"kullback-leibler\",\n",
    "    solver=\"mu\",\n",
    "    max_iter=1000,\n",
    "    alpha=0.1,\n",
    "    l1_ratio=0.5,\n",
    ").fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(\n",
    "    nmf,\n",
    "    tfidf_feature_names,\n",
    "    n_top_words,\n",
    "    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
